{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fVACAHzCpP5n"
   },
   "source": [
    "# Image Classification with PyTorch: AlexNet and ResNet\n",
    "\n",
    "[Pablo Carballeira] (http://dymas.ii.uam.es/webvpu/gti/user/186/), Escuela Politecnica Superior, Universidad Aut√≥noma de Madrid.\n",
    "\n",
    "Parts of this code have been adapted from then work of Kevin McGuinness (http://www.eeng.dcu.ie/~mcguinne/), School of Electronic Engineering, Dublin City University, and the work of Ben Trevett (https://github.com/bentrevett), Heriot-Watt University\n",
    "\n",
    "You can find documentation about working in Colab here (https://colab.research.google.com/notebooks/intro.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "In this lab assignment you will learn how to use the [PyTorch](https://pytorch.org/) deep learning framework to create and train a modified version of the AlexNet model and a simplified ResNet model for image classification. You will use the [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset that contains 10 classes of color images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGmS80CT8aCw"
   },
   "source": [
    "# Instructions\n",
    "\n",
    "Anywhere you see a **???** in the code below, fill in in with the correct code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ywspo1LiklkS"
   },
   "source": [
    "# Import packages\n",
    "\n",
    "Find the PyTorch docs at https://pytorch.org/docs/stable/index.html \n",
    "\n",
    "Tutorials: https://pytorch.org/tutorials/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XYaxtUEgIafG"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn import manifold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-MGvs0jJ0JgA"
   },
   "source": [
    "# Enable GPU acceleration\n",
    "\n",
    "Open to the Edit menu and select *Notebook settings* and check that *GPU* is selected under hardware accelerator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2OSN5zLGLxkF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# make sure to enable GPU acceleration!\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FcvzjWq-0xs"
   },
   "source": [
    "We set the random seed so all of our experiments can be reproduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "QfZxvR09IafJ"
   },
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eBug0gKGhA9x"
   },
   "source": [
    "# Dataset: CIFAR10\n",
    "\n",
    "We are now moving on from the MNIST dataset and we will be using the [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset. CIFAR10 consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. The classes are: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck. \n",
    "\n",
    "![](https://github.com/bentrevett/pytorch-image-classification/blob/master/assets/cifar10.png?raw=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvLlXb9vhsMV"
   },
   "source": [
    "# Dataset pre-processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uecfsf1f-0xw"
   },
   "source": [
    "## Mean and standard deviation of the dataset\n",
    "\n",
    "As mentioned in the previous notebook, for those models that do not use early batch normalization, it is important to transform the dataset so that the images have mean zero and unit variance. \n",
    "\n",
    "Here, we calculate the mean and standard deviation of our data so we can normalize it appropiately. CIFAR10 is made up of color images with three color channels (red, green and blue). To normalize the data we need to calculate the means and standard deviations for each of the color channels independently. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IprUgI0fIafN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Calculated means: [0.49139968 0.48215841 0.44653091]\n",
      "Calculated stds: [0.24703223 0.24348513 0.26158784]\n"
     ]
    }
   ],
   "source": [
    "ROOT = '.data/CIFAR10'\n",
    "\n",
    "train_data = datasets.CIFAR10(root = ROOT, \n",
    "                              train = True, \n",
    "                              download = True)\n",
    "\n",
    "# use the functions that are built in the dataset to obtain the means and stds. \n",
    "# Bear in mind that image pixels are in the [0,255] range, but they are converted to\n",
    "# the [0,1] range when transformed to Tensor variables \n",
    "means = train_data.data.mean(axis = (0,1,2)) / 255\n",
    "stds = train_data.data.std(axis = (0,1,2)) / 255\n",
    "\n",
    "print(f'Calculated means: {means}')\n",
    "print(f'Calculated stds: {stds}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajVaWn_7-0x0"
   },
   "source": [
    "\n",
    "Next up is defining the transforms for the training and validation sets. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "tfzKdrpfIafR"
   },
   "outputs": [],
   "source": [
    "# define the train and tesst transforms with normalization with the means and \n",
    "# stds we have just computed \n",
    "train_transforms = transforms.Compose([\n",
    "   transforms.ToTensor(),\n",
    "   transforms.Normalize(mean=means,\n",
    "                        std=stds)\n",
    "])\n",
    "\n",
    "valid_transforms = train_transforms = transforms.Compose([\n",
    "   transforms.ToTensor(),\n",
    "   transforms.Normalize(mean=means,\n",
    "                        std=stds)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TPLqEffl-0x3"
   },
   "source": [
    "Next, as standard, we'll load the dataset with our transforms..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "vqxev2wBIafU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_set = datasets.CIFAR10(root = ROOT, \n",
    "                              train = True, \n",
    "                              download = True,\n",
    "                              transform = train_transforms)\n",
    "valid_set =datasets.CIFAR10(root = ROOT, \n",
    "                              train = True, \n",
    "                              download = True,\n",
    "                              transform = valid_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjflicvF-0yB"
   },
   "source": [
    "We print out the number of examples in each set of data to ensure everything has gone OK so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "enX-chu-Iafc"
   },
   "outputs": [],
   "source": [
    "print(f'Number of training examples: {len(train_set)}')\n",
    "print(f'Number of validation examples: {len(valid_set)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQq6BaNr-0yE"
   },
   "source": [
    "## Dataset examples\n",
    "\n",
    "We will use `matplotlib` to see some examples of the CIFAR10 dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAFVQhu4-0yN"
   },
   "source": [
    "`matplotlib` expects the values of every pixel to be between $[0, 1]$. However our normalization will cause them to be outside this range. A solution to this is to renormalize the images so each pixel is between $[0, 1]$, so we'll write a function that does it and we can use whenever we need to renormalize an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gVDM7PTCswBx"
   },
   "outputs": [],
   "source": [
    "def normalize_image(image):\n",
    "    image_min = image.min()\n",
    "    image_max = image.max()\n",
    "    image.clamp_(min = image_min, max = image_max)\n",
    "    image.add_(-image_min).div_(image_max - image_min + 1e-5)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcO-86uQOVxA"
   },
   "source": [
    "Now, we'll create a function to plot some of the images in our dataset to see what they actually look like.\n",
    "\n",
    "Note that by default PyTorch handles images that are arranged `[channel, height, width]`, but `matplotlib` expects images to be `[height, width, channel]`, hence we need to `permute` our images before plotting them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KVwZrsXpcUSB"
   },
   "outputs": [],
   "source": [
    "def plot_images(images, labels, classes, normalize = False):\n",
    "\n",
    "    n_images = len(images)\n",
    "\n",
    "    rows = int(np.sqrt(n_images))\n",
    "    cols = int(np.sqrt(n_images))\n",
    "\n",
    "    fig = plt.figure(figsize = (10, 10))\n",
    "\n",
    "    for i in range(rows*cols):\n",
    "\n",
    "        ax = fig.add_subplot(rows, cols, i+1)\n",
    "        \n",
    "        image = images[i]\n",
    "        image = normalize_image(image)\n",
    "\n",
    "        ax.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "        ax.set_title(classes[labels[i]])\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXe4GSO7-0yH"
   },
   "source": [
    "Then, we'll actually plot the images.\n",
    "\n",
    "We get both the images and the labels from the training set and convert the labels, which are originally stored as integers, into their human readable class by using the data's `classes` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6piM5oUcZIY"
   },
   "outputs": [],
   "source": [
    "N_IMAGES = 25\n",
    "\n",
    "images, labels = zip(*[(image, label) for image, label in \n",
    "                           [train_set[i] for i in range(N_IMAGES)]])\n",
    "classes = valid_set.classes\n",
    "plot_images(images, labels, classes, normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BL_2X7ZF-0yl"
   },
   "source": [
    "The final bit of the data processing is creating the iterators.\n",
    "\n",
    "We will use here a larger batch size. Generally, when using a GPU, a larger batch size means our model trains faster. Our model has significantly more parameters and the images it is training on are much larger, than the previous notebook, so will generally take longer. We offset this as much as we can by using a batch size of 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o3rhhD0HIaff"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "train_loader = ???\n",
    "valid_loader = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NF2IR5u8-0xo"
   },
   "source": [
    "# AlexNet\n",
    "\n",
    "In this notebook we will be implementing a modified version of [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf), a neural network model that uses convolutional neural network (CNN) layers and was designed for the [ImageNet challenge](http://www.image-net.org/challenges/LSVRC/). AlexNet is famous for winning the ImageNet challenge in 2012 by beating the second place competitor by over 10% accuracy and kickstarting the interest in deep learning for computer vision.\n",
    "\n",
    "The image below shows the original architecture of AlexNet. There are two \"paths\" of processing through the network. This is due to the original AlexNet model being implemented on two GPUs in parallel. Almost all implementations of AlexNet are now on a single GPU and our implementation is too. In a single-GPU implementation, both paths are combined by doubling the number of feature channels in each layer (adds up the number of feature channels in each path)\n",
    "\n",
    "![](https://github.com/bentrevett/pytorch-image-classification/blob/master/assets/alexnet.png?raw=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98q8mpRN-0yp"
   },
   "source": [
    "# Defining the Model\n",
    "\n",
    "The AlexNet model itself is not conceptually very different from the simple CNN model of the previous notebook. It is made up of convolutional layers, pooling layers, ReLU activation functions and Dropout layers.\n",
    "\n",
    "AlexNet was designed for ImageNet, that comprises 224x224 color images. Here, we are using 32x32 images of the CIFAR10 dataset, so we will adapt the original AlexNet architecture with the following modifications:\n",
    "\n",
    "* the inputs are 32x32 color images\n",
    "*  only one branch of the model will be implemented, so the layers will have half the number of channels than in the complete version (the complete version doubles the number of channels in a single branch)\n",
    "*  5x5 kernels are used in the first conv layer, instead of the original 11x11 kernel size\n",
    "* 2x2 max pooling kernels are used\n",
    "*  Local Response Normalization layers are skipped \n",
    "*  Dropout with probability 0.5 is added before the first and second Linear layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bcEh5PbCS7Zf"
   },
   "source": [
    "## Model definition: Sequential \n",
    "\n",
    "`Sequential` is used to compact the code for the definition of the network. We use `Sequential` to define the multiple layers of the CNN and when the `Sequential` module is called it will apply each layer, in order, to the input. There is no difference between using a `Sequential` and having each module defined in the `__init__` and then called in `forward` - however it makes the code significantly shorter.\n",
    "\n",
    "We will create one `Sequential` model, `features`, for all the convolutional and pooling layers, then we flatten the data, and pass it to another `Sequential` model, `classifier`,  which is made up of linear layers. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6v164iGLTQnR"
   },
   "source": [
    "One last thing to mention is that the very first convolutional layer has an `in_channel` of three. That is because we are handling color images that have three channels (red, green and blue) instead of the single channel grayscale images from the MNIST dataset. This doesn't change the way any of the convolutional filter works, it just means the first filter has a depth of three instead of a depth of one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDUPbVhzj-ym"
   },
   "source": [
    "\n",
    "Note that the model is defined with the number of output classes as a parameter (defined by the number of classes of the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iwl1iL2llQT6"
   },
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            # First convolutional layer. Use 5x5 kernel instead of 11x11\n",
    "            nn.Conv2d(3, 48, 5, 2, 2), #in_channels, out_channels, kernel_size, stride, padding\n",
    "            nn.MaxPool2d(2), #kernel_size\n",
    "            nn.ReLU(inplace = True),\n",
    "            # Complete the following four conv layers of the AlexNet model. \n",
    "            # Subsampling is only performed by 2x2 max pooling layers (not with stride in the \n",
    "            # convolutional layers)\n",
    "            # Pay special attention to the number of input and output channels of each layer\n",
    "            ???\n",
    "          \n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            # First linear layer\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128 * 2 * 2, 2048), # final conv layer resolution 2x2\n",
    "            nn.ReLU(inplace = True),\n",
    "            # second linear layer\n",
    "            ???\n",
    "            \n",
    "            # Last Linear layer. No ReLU\n",
    "            nn.Linear(2048, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        interm_features = x.view(x.shape[0], -1)\n",
    "        x = self.classifier(interm_features)\n",
    "        return x, interm_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxwFHUBt6lMd"
   },
   "source": [
    "The code below is used to check that the model architecture is defined correctly. The code should not return any error, and the output should be the following: \n",
    "\n",
    "```\n",
    "Output size:  torch.Size([1, 5])\n",
    "Output: tensor([[ 0.0021, -0.0076,  0.0116,  0.0238, -0.0155], grad_fn=<AddmmBackward>)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Skxdd0E6j3S"
   },
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "dummy_input = torch.rand(1,3,32,32)\n",
    "model = AlexNet(5)\n",
    "output = model(dummy_input)\n",
    "\n",
    "print('Output size: ',format(output[0].shape))\n",
    "print('Output:')\n",
    "print(output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBMS4TH4XVow"
   },
   "source": [
    "We'll create an instance of our model with the desired amount of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y-sDcYnBIafp"
   },
   "outputs": [],
   "source": [
    "model = ???\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9Dkc_uBXVoy"
   },
   "source": [
    "Then we'll see how many trainable parameters our model has. AlexNet is a relatively small model for computer vision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3g-pV7t2Iafs"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n99m5MPZXVo1"
   },
   "source": [
    "# Training the Model\n",
    "\n",
    "Use the code from previous notebook to train the model for 15 epochs, using the same parameters for the optimizer, and plot the learning curves\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dpSfdW2uqGDv"
   },
   "outputs": [],
   "source": [
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TbnXRjM9IMSs"
   },
   "source": [
    "# Error analysis\n",
    "\n",
    "Here we will take a look at the validation samples that the model got wrong. This is often a good idea when trying to improve a models, since it gives you some intuition about the kinds of examples the model finds difficult, which can lead to insights on how to improve models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-gBJCx29mjn"
   },
   "source": [
    "We'll examine our model by: plotting a confusion matrix, seeing which incorrect examples our model was most confident about, viewing our model's learned representations in two dimensions with t-SNE, and taking a look at the weights of our model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5k7iNHQ_xLbr"
   },
   "source": [
    "**Note**: The code in the previous notebook should help you to easily complete the missing parts in this Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zg0yyhz-t77M"
   },
   "source": [
    "## Predictions of the model in the validation set\n",
    "\n",
    "We start by defining a predict function which takes a data loader and produces predictions for all samples.\n",
    "\n",
    "In addition to the predictions, this function also provides other useful outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w6PJMaehrEC3"
   },
   "outputs": [],
   "source": [
    "def get_predictions(model, iterator, device):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # save the predictions, images and labels in this list\n",
    "    y_preds = []\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch, targets in iterator:\n",
    "            batch = batch.to(device)\n",
    "            # predict probabilities of each class\n",
    "            predictions, _ = model(batch)\n",
    "            # apply a softmax to the predictions\n",
    "            predictions = ???\n",
    "            \n",
    "            # top_pred = y_preds.argmax(1, keepdim = True)\n",
    "\n",
    "            # save\n",
    "            images.append(batch.cpu())\n",
    "            labels.append(targets.cpu())\n",
    "            y_preds.append(predictions.cpu())\n",
    "\n",
    "    # stack\n",
    "    images = torch.cat(images, dim = 0)\n",
    "    labels = torch.cat(labels, dim = 0)\n",
    "    y_preds = torch.cat(y_preds, dim = 0)\n",
    "\n",
    "    return images, labels, y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8nzP0-he-4i_"
   },
   "outputs": [],
   "source": [
    "# compute predictions on the validation set\n",
    "# images, labels, probs =\n",
    "images, labels, probs = get_predictions(model, valid_loader, device)\n",
    "\n",
    "# ...and then get the predicted labels from the model's predictions. Use torch.argmax\n",
    "pred_labels = ???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "87r7l8ewG6v-"
   },
   "outputs": [],
   "source": [
    "# convert to numpy\n",
    "y_true = np.array(labels)\n",
    "y_pred = np.array(pred_labels)\n",
    "\n",
    "# calculate the number of errors\n",
    "num_errors = ???\n",
    "\n",
    "print(f'Validation errors {num_errors} (out of {len(valid_set)})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSoN7mTdq_WA"
   },
   "source": [
    "## Confusion matrix\n",
    " \n",
    "We'll examine our model by plotting a confusion matrix, which helps us understand which classes the model is more frequently mixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hNTblBsmYFeM"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(labels, pred_labels, classes):\n",
    "    \n",
    "    fig = plt.figure(figsize = (10, 10));\n",
    "    ax = fig.add_subplot(1, 1, 1);\n",
    "    cm = confusion_matrix(labels, pred_labels);\n",
    "    cm = ConfusionMatrixDisplay(cm, display_labels = classes);\n",
    "    cm.plot(values_format = 'd', cmap = 'Blues', ax = ax)\n",
    "    plt.xticks(rotation = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9hIoIpNNYFeS"
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(labels, pred_labels, classes) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9Pj6JjvkjAw"
   },
   "source": [
    "## Analysis of samples predicted wrongly\n",
    "Here we will find the samples that the model is predicting wrongly and we will. Especifically, we will look at those samples that the model is predicting wrongly with a high score for the wrong class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVVbhlf7YFeV"
   },
   "source": [
    "We can then find which predictions were correct and then sort the incorrect predictions in descending order of their confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D685S9yxYFeV"
   },
   "outputs": [],
   "source": [
    "corrects = torch.eq(labels, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EYMclpbxYFeY"
   },
   "outputs": [],
   "source": [
    "incorrect_examples = []\n",
    "\n",
    "for image, label, prob, correct in zip(images, labels, probs, corrects):\n",
    "    if not correct:\n",
    "        incorrect_examples.append((image, label, prob))\n",
    "\n",
    "incorrect_examples.sort(reverse = True, key = lambda x: torch.max(x[2], dim = 0).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1HAtUEsSYFea"
   },
   "outputs": [],
   "source": [
    "def plot_most_incorrect(incorrect, classes, n_images, normalize = True):\n",
    "\n",
    "    rows = int(np.sqrt(n_images))\n",
    "    cols = int(np.sqrt(n_images))\n",
    "\n",
    "    fig = plt.figure(figsize = (25, 20))\n",
    "\n",
    "    for i in range(rows*cols):\n",
    "\n",
    "        ax = fig.add_subplot(rows, cols, i+1)\n",
    "        \n",
    "        image, true_label, probs = incorrect[i]\n",
    "        image = image.permute(1, 2, 0)\n",
    "        true_prob = probs[true_label]\n",
    "        incorrect_prob, incorrect_label = torch.max(probs, dim = 0)\n",
    "        true_class = classes[true_label]\n",
    "        incorrect_class = classes[incorrect_label]\n",
    "\n",
    "        if normalize:\n",
    "            image = normalize_image(image)\n",
    "\n",
    "        ax.imshow(image.cpu().numpy())\n",
    "        ax.set_title(f'true label: {true_class} ({true_prob:.3f})\\n' \\\n",
    "                     f'pred label: {incorrect_class} ({incorrect_prob:.3f})')\n",
    "        ax.axis('off')\n",
    "        \n",
    "    fig.subplots_adjust(hspace = 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_SNmUtnYFed"
   },
   "outputs": [],
   "source": [
    "N_IMAGES = 36\n",
    "\n",
    "plot_most_incorrect(incorrect_examples, classes, N_IMAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ahtWY4LzQL43"
   },
   "source": [
    "# T-SNE representations: final  layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plZRbvc1riVT"
   },
   "outputs": [],
   "source": [
    "def get_representations(model, iterator, device):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    outputs = []\n",
    "    intermediates = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for (x, y) in iterator:\n",
    "\n",
    "            x = x.to(device)\n",
    "\n",
    "            y_pred, interm_feats = model(x)\n",
    "\n",
    "            outputs.append(y_pred.cpu())\n",
    "            intermediates.append(interm_feats.cpu())\n",
    "            labels.append(y)\n",
    "        \n",
    "    outputs = torch.cat(outputs, dim = 0)\n",
    "    intermediates = torch.cat(intermediates, dim = 0)\n",
    "    labels = torch.cat(labels, dim = 0)\n",
    "\n",
    "    return outputs, intermediates, labels\n",
    "    # return outputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3vPRW_BIrkQT"
   },
   "outputs": [],
   "source": [
    "outputs, intermediates, labels = get_representations(model, valid_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3mnBYh1kd_F_"
   },
   "outputs": [],
   "source": [
    "def plot_representations(data, labels, classes, n_images = None):\n",
    "    \n",
    "    if n_images is not None:\n",
    "        data = data[:n_images]\n",
    "        labels = labels[:n_images]\n",
    "        \n",
    "    fig = plt.figure(figsize = (10, 10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    scatter = ax.scatter(data[:, 0], data[:, 1], c = labels, cmap = 'tab10')\n",
    "    handles, labels = scatter.legend_elements()\n",
    "    legend = ax.legend(handles = handles, labels = classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "duqrChdkd_GD"
   },
   "outputs": [],
   "source": [
    "def get_tsne(data, n_components = 2, n_images = None):\n",
    "    \n",
    "    if n_images is not None:\n",
    "        data = data[:n_images]\n",
    "        \n",
    "    tsne = manifold.TSNE(n_components = n_components, random_state = 0)\n",
    "    tsne_data = tsne.fit_transform(data)\n",
    "    return tsne_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fdZGF_76d_GF"
   },
   "outputs": [],
   "source": [
    "N_IMAGES = 5_000\n",
    "\n",
    "output_tsne_data = get_tsne(outputs, n_images = N_IMAGES)\n",
    "plot_representations(output_tsne_data, labels, classes, n_images = N_IMAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nyh_skghXVqM"
   },
   "source": [
    "# Visualization of filters \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjVyMM9xXVqR"
   },
   "source": [
    "Next, we'll plot some images after they have been convolved with the first convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i-iX81gAYr1U"
   },
   "outputs": [],
   "source": [
    "def plot_filtered_images(images, filters, n_filters = None, normalize = True):\n",
    "\n",
    "    images = torch.cat([i.unsqueeze(0) for i in images], dim = 0).cpu()\n",
    "    filters = filters.cpu()\n",
    "\n",
    "    if n_filters is not None:\n",
    "        filters = filters[:n_filters]\n",
    "\n",
    "    n_images = images.shape[0]\n",
    "    n_filters = filters.shape[0]\n",
    "\n",
    "    filtered_images = F.conv2d(images, filters)\n",
    "\n",
    "    fig = plt.figure(figsize = (30, 30))\n",
    "\n",
    "    for i in range(n_images):\n",
    "\n",
    "        image = images[i]\n",
    "\n",
    "        if normalize:\n",
    "            image = normalize_image(image)\n",
    "\n",
    "        ax = fig.add_subplot(n_images, n_filters+1, i+1+(i*n_filters))\n",
    "        ax.imshow(image.permute(1,2,0).numpy())\n",
    "        ax.set_title('Original')\n",
    "        ax.axis('off')\n",
    "\n",
    "        for j in range(n_filters):\n",
    "            image = filtered_images[i][j]\n",
    "\n",
    "            if normalize:\n",
    "                image = normalize_image(image)\n",
    "\n",
    "            ax = fig.add_subplot(n_images, n_filters+1, i+1+(i*n_filters)+j+1)\n",
    "            ax.imshow(image.numpy(), cmap = 'bone')\n",
    "            ax.set_title(f'Filter {j+1}')\n",
    "            ax.axis('off');\n",
    "\n",
    "    fig.subplots_adjust(hspace = -0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LjY35yaoZCmE"
   },
   "outputs": [],
   "source": [
    "N_IMAGES = 5\n",
    "N_FILTERS = 7\n",
    "\n",
    "images = []\n",
    "images = [image for image, label in [valid_set[i] for i in range(N_IMAGES)]]\n",
    "filters = model.features[0].weight.data\n",
    "\n",
    "plot_filtered_images(images, filters, N_FILTERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5y92TbXXVqS"
   },
   "source": [
    "We can see different types of edge detection and blurring that the filters have learned that are apparently decent feature extractors for this model and task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2YFr4T1YXVqW"
   },
   "source": [
    "Finally, we can plot the actual filters our model has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ieaJiA8D7bpT"
   },
   "outputs": [],
   "source": [
    "def plot_filters(filters, normalize = True):\n",
    "\n",
    "    filters = filters.cpu()\n",
    "\n",
    "    n_filters = filters.shape[0]\n",
    "\n",
    "    rows = int(np.sqrt(n_filters))\n",
    "    cols = int(np.sqrt(n_filters))\n",
    "\n",
    "    fig = plt.figure(figsize = (20, 10))\n",
    "\n",
    "    for i in range(rows*cols):\n",
    "\n",
    "        image = filters[i]\n",
    "\n",
    "        if normalize:\n",
    "            image = normalize_image(image)\n",
    "\n",
    "        ax = fig.add_subplot(rows, cols, i+1)\n",
    "        ax.imshow(image.permute(1, 2, 0))\n",
    "        ax.axis('off')\n",
    "\n",
    "    fig.subplots_adjust(wspace = -0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SU214ywn8sKJ"
   },
   "outputs": [],
   "source": [
    "plot_filters(filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRIC4tEgXVqY"
   },
   "source": [
    "Sadly, there is nothing really interpretable here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9UYRoAx1VM7t"
   },
   "source": [
    "Can we learn interesting looking filters? Or are we doomed to just look at colorful blocks?\n",
    "\n",
    "Well, we can take a *pre-trained* AlexNet model and view the filters of that. This pre-trained version of AlexNet was trained by people at PyTorch and was not trained on the CIFAR10 dataset, but on the [ILSVRC](https://arxiv.org/abs/1409.0575) dataset, usually just called the ImageNet dataset. ImageNet is a dataset with over 1 million images in 1,000 classes. Torchvision provides ways of downloading different models pre-trained on ImageNet, such as AlexNet and [many others](https://pytorch.org/docs/stable/torchvision/models.html).  \n",
    "\n",
    "First, we can import the model making sure to pass `pretrained = True` to get a pre-trained version. Torchvision will then import the model, download the weights for it and load them into the new model.\n",
    "\n",
    "We can see that this is similar to our AlexNet model but has considerably more parameters.\n",
    "\n",
    "One interesting thing is that they use much larger filters in the first convolutional layer - 11x11 instead of 5x5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5I642oecVKso"
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "model = models.alexnet(pretrained = True)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8E9GGKDYaEa"
   },
   "source": [
    "We can then get the learned values of these filters the same way we did for our version of AlexNet and then plot them.\n",
    "\n",
    "As we can see the patterns are much more interesting. So how come it learned these interesting looking filters? Is it just because the filters are bigger? Is it because models can only do well on ImageNet if they learn these types of filters? Do these more interesting looking filters imply that they perform better? Or are these filters showing how the model has overfit to patterns on the images within ImageNet?\n",
    "\n",
    "It is difficult to answer these questions, and network interpretability is one of the still open questions in the research community. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-KUGUuxZYOZI"
   },
   "outputs": [],
   "source": [
    "filters_imagenet = model.features[0].weight.data\n",
    "\n",
    "plot_filters(filters_imagenet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdQg7Bc9IKIf"
   },
   "source": [
    "# Residual Networks: Basic Residual Network\n",
    "\n",
    "Finally, we will implement a basic residual network with skip connections between layers.  We will evaluate its performance over the same dataset and compare it with the non-residual architecture (Alexnet), in terms of performance and network complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-d1wtVCjxTzb"
   },
   "source": [
    "## Residual connections\n",
    "\n",
    "A residual connection is simply a direct connection between the input of a block and the output of a block. Sometimes the residual connection has layers in it, but most of the time it does not. Below is an example block with an identity residual connection, i.e. no layers in the residual path.\n",
    "\n",
    "![](https://github.com/bentrevett/pytorch-image-classification/blob/master/assets/resnet-skip.png?raw=1)\n",
    "\n",
    "Why do ResNets work? The key is in the residual connections. Training incredibly deep neural networks is difficult due to the gradient signal either exploding (becoming very large) or vanishing (becoming very small) as it gets backpropagated through many layers. Residual connections allow the model to learn how to \"skip\" layers - by setting all their weights to zero and only rely on the residual connection. Thus, in theory, if your ResNet152 model can actually learn the desired function between input and output by only using the first 52 layers the remaining 100 layers should set their weights to zero and the output of the 52nd layer will simply pass through the residual connections unhindered. This also allows for the gradient signal to also backpropagate through those 100 layers unhindered too. In theory, this outcome could also also be achieved in a network without residual connections: the \"skipped\" layers would learn to set their weights to one, however adding the residual connection is more explicit and is easier for the model to learn to use these residual connections.\n",
    "\n",
    "The image below shows a comparison between VGG-19, a convolutional neural network architecture without residual connections, and one with residual connections - ResNet34. \n",
    "\n",
    "![](https://github.com/bentrevett/pytorch-image-classification/blob/master/assets/vgg-resnet.png?raw=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaAWCBrWIWuQ"
   },
   "source": [
    "# Defining the Model\n",
    "![resnet.png](data:image/png;base64,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)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sh17_aNlIWuR"
   },
   "source": [
    "We will implement a similar architecture to the ResNet shown in the figure above. Our simplified model will have the following differences:\n",
    "\n",
    "* We will skip the initial 7x7 and pooling layers\n",
    "* We will use 8 3x3 conv layers stacked as shown in the figure\n",
    "* The green blocks correspond to residual blocks with additional downsampling (downsampling blocks). The first convolutional layer downsamples the input by a factor of 2, using the stride parameter, and sets the number of feature channels. Also, in these blocks, the input needs to be resized to be added to the output \n",
    "* The pink residual blocks maintain all dimensions unaltered \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2kXd_IHRX7o"
   },
   "source": [
    "Let's define the basic residual block. We have two options:\n",
    "* Downsampling blocks: the input is filtered by a 1x1 convolution that also reduces the spatial resolution by a factor of 2.\n",
    "* Non-downsampling: spatial and channel resolutions are mantained\n",
    "\n",
    "Note that we use Batch Normalization after each convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M01pjMnC_KcP"
   },
   "outputs": [],
   "source": [
    "class BasicResBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride = 1, padding = 1 , downsample = False):\n",
    "        super().__init__()\n",
    "         \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, \n",
    "                               stride, padding, bias = False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        \n",
    "        # define the second conv and BN layers. This second layer does not change any dimensions in any case\n",
    "        self.conv2 = ???\n",
    "        self.bn2 = ???\n",
    "                \n",
    "        if downsample:\n",
    "            # define a 1x1 convolutional layer for the input, that also reduces the spatial resolution\n",
    "            # using the same stride value used in the conv1 layer\n",
    "            conv1D = ??? \n",
    "            \n",
    "            bn = nn.BatchNorm2d(out_channels)\n",
    "            downsample = nn.Sequential(conv1D, bn)\n",
    "        else:\n",
    "            downsample = None\n",
    "        \n",
    "        self.downsample = downsample\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "       # we save the input to add it later (skip connection)        \n",
    "        i = x\n",
    "        \n",
    "        # apply conv, bn, and relu layers to the input in the same order \n",
    "        # that they are defined\n",
    "        # x = ???\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        \n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            i = self.downsample(i)\n",
    "                        \n",
    "        # add input (implement skip connection) and a final relu layer                \n",
    "        # x = ???\n",
    "        x += i\n",
    "        x = self.relu(x)\n",
    "                \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpAIxiYBIWuZ"
   },
   "source": [
    "\n",
    "We will use the Basic Residual Block to implement the architecture of the Basic Residual Network  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GyBKmTSts21v"
   },
   "outputs": [],
   "source": [
    "class BasicResNet(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # define the 8 convolutional layers using the BasicResBlock (4 blocks)\n",
    "        ???\n",
    "        \n",
    "        # define an adaptive average pooling layer with an output size of 2x2\n",
    "        # use nn.AdaptiveAvgPool2d\n",
    "        self.avgpool= ???\n",
    "               \n",
    "        # classifier with 1 layer\n",
    "        self.classifier = nn.Sequential(\n",
    "            # First linear layer\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64 * 2 * 2, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        \n",
    "        interm_features = x.view(x.shape[0], -1)\n",
    "        x = self.classifier(interm_features)\n",
    "        return x, interm_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMFDYPo7_EpS"
   },
   "source": [
    "The code below is used to check that the model architecture is defined correctly. The code should not return any error, and the output should be the following: \n",
    "\n",
    "```\n",
    "Output size:  torch.Size([1, 5])\n",
    "Output: tensor([[-1.3275, -1.6205,  0.5323,  0.4632, -0.4157]], grad_fn=<AddmmBackward>)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lUrag_mQ_EpS"
   },
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "dummy_input = torch.rand(1,3,32,32)\n",
    "model = BasicResNet(5)\n",
    "output = model(dummy_input)\n",
    "\n",
    "print('Output size: ',format(output[0].shape))\n",
    "print('Output:')\n",
    "print(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PuJ3UnD-NjMu"
   },
   "outputs": [],
   "source": [
    "model = ???\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mo-Q8eViDSE4"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0iFC4XnNJgm"
   },
   "source": [
    "# Training the Model\n",
    "\n",
    "Use the code from previous notebook to train the model for 15 epochs **(with a learning rate of 0.005)** and plot the learning curves\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DsHZinyYjoGN"
   },
   "outputs": [],
   "source": [
    "???"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "[UAM-DL4IP] 1.Image Classification (2).ipynb",
   "provenance": [
    {
     "file_id": "1dl2xGhhuRL1CXR1ts207qzF8wVHSWlJc",
     "timestamp": 1602836929192
    },
    {
     "file_id": "1wAXQg0DIlaFIElZWSUqu4YUjpK16qp0A",
     "timestamp": 1602254884643
    },
    {
     "file_id": "https://github.com/bentrevett/pytorch-image-classification/blob/master/3_alexnet.ipynb",
     "timestamp": 1599039771698
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
