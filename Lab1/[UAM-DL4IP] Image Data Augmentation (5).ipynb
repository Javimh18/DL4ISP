{"cells":[{"cell_type":"markdown","metadata":{"id":"fVACAHzCpP5n"},"source":["# Image Classification with PyTorch: Trying different image data augmentations when training a CNN\n","\n","[Pablo Carballeira] (http://dymas.ii.uam.es/webvpu/gti/user/186/), Escuela Politecnica Superior, Universidad Aut√≥noma de Madrid.\n","\n","Parts of this code have been adapted from then work of Kevin McGuinness (http://www.eeng.dcu.ie/~mcguinne/), School of Electronic Engineering, Dublin City University, and the work of Ben Trevett (https://github.com/bentrevett), Heriot-Watt University\n","\n","You can find documentation about working in Colab here (https://colab.research.google.com/notebooks/intro.ipynb)\n","\n","---\n","\n","In this lab assignment you will try different image data augmentation techniques to train a ResNet CNN on the CIFAR10 datastet. The objective of this assignment is that you try different data augmentation options and evaluate which is more efficient for your problem."]},{"cell_type":"markdown","metadata":{"id":"FGmS80CT8aCw"},"source":["# Instructions\n","\n","Go to the Section \"Load data and define the image transformations\" and modify it to try different image data augmentation techniques. You can also play with different values of the parameters of the transformations you utilize.\n","\n","Each time you set a new set of transformations (training and validation), you have to run the whole notebook to evaluate its influence in the training process\n"]},{"cell_type":"markdown","metadata":{"id":"Ywspo1LiklkS"},"source":["# Import packages\n","\n","Find the PyTorch docs at https://pytorch.org/docs/stable/index.html \n","\n","Tutorials: https://pytorch.org/tutorials/"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"XYaxtUEgIafG"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import _LRScheduler\n","import torch.utils.data as data\n","\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","\n","from sklearn import decomposition\n","from sklearn import manifold\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import ConfusionMatrixDisplay\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","import copy\n","import random\n","import time"]},{"cell_type":"markdown","metadata":{"id":"-MGvs0jJ0JgA"},"source":["# Enable GPU acceleration\n","\n","Open to the Edit menu and select *Notebook settings* and check that *GPU* is selected under hardware accelerator.\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"2OSN5zLGLxkF"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using mps device\n"]}],"source":["# make sure to enable GPU acceleration!\n","device = (\n","    \"cuda\"\n","    if torch.cuda.is_available()\n","    else \"mps\"\n","    if torch.backends.mps.is_available()\n","    else \"cpu\"\n",")\n","print(f\"Using {device} device\")"]},{"cell_type":"markdown","metadata":{"id":"1FcvzjWq-0xs"},"source":["We set the random seed so all of our experiments can be reproduced."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"QfZxvR09IafJ"},"outputs":[],"source":["SEED = 1234\n","\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True"]},{"cell_type":"markdown","metadata":{"id":"eBug0gKGhA9x"},"source":["# Dataset: CIFAR10\n","\n","![](https://github.com/bentrevett/pytorch-image-classification/blob/master/assets/cifar10.png?raw=1)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"uecfsf1f-0xw"},"source":["## Mean and standard deviation of the dataset\n","\n","Here, we calculate the mean and standard deviation of our data so we can normalize it appropiately. CIFAR10 is made up of color images with three color channels (red, green and blue). To normalize the data we need to calculate the means and standard deviations for each of the color channels independently. \n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"IprUgI0fIafN"},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","Calculated means: [0.49139968 0.48215841 0.44653091]\n","Calculated stds: [0.24703223 0.24348513 0.26158784]\n"]}],"source":["ROOT = '.data/CIFAR10'\n","\n","train_data = datasets.CIFAR10(root = ROOT, \n","                              train = True, \n","                              download = True)\n","\n","# use the functions that are built in the dataset to compute the means and stds. \n","# Bear in mind that image pixels are in the [0,255] range, but they are converted to\n","# het [0,1] range when transformed to Tensor variables \n","means = train_data.data.mean(axis = (0,1,2)) / 255\n","stds = train_data.data.std(axis = (0,1,2)) / 255\n","\n","print(f'Calculated means: {means}')\n","print(f'Calculated stds: {stds}')"]},{"cell_type":"markdown","metadata":{"id":"fdQg7Bc9IKIf"},"source":["# CNN architecture: Residual Network\n","\n","We will use a 11-layer ResNet model adapted to the resolution of the images of the CIFAR10 dataset. Here we define the network architecture and instantiate the model with random weights. \n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"YEoGJPxsuuSj"},"outputs":[],"source":["class _Block(nn.Module):\n","    def __init__(self, kernel_dim, n_filters, in_channels, stride, padding):\n","        super(_Block, self).__init__()\n","        self._layer_one = nn.Conv2d(in_channels=in_channels, out_channels=n_filters, \n","                                  kernel_size=kernel_dim, stride=stride[0], padding=padding, bias=False)\n","        self._layer_two = nn.Conv2d(in_channels=n_filters, out_channels=n_filters, \n","                                  kernel_size=kernel_dim, stride=stride[1], padding=padding, bias=False)\n","        self.relu = nn.ReLU()\n","        self.bn1 = nn.BatchNorm2d(n_filters)\n","        self.bn2 = nn.BatchNorm2d(n_filters)\n","    def forward(self, X, shortcut = None):\n","        output = self._layer_one(X)\n","        output = self.bn1(output)\n","        output = self.relu(output)\n","        output = self._layer_two(output)\n","        output = self.bn2(output)\n","        output = self.relu(output)\n","        if isinstance(shortcut, torch.Tensor):\n","            return output + shortcut\n","        return output + X\n","class ResNet(nn.Module):\n","    def __init__(self, input_dim, n_classes):\n","        super(ResNet, self).__init__()\n","        self.n_classes = n_classes\n","        self.conv1 = nn.Conv2d(3, 64, 4, 2)\n","        self.block1 = _Block(5, 64, 64, (1,1), 2)\n","        self.block2 = _Block(5, 64, 64, (1,1), 2)\n","        self.block3 = _Block(5, 64, 64, (1,1), 2)\n","        self.transition1 = nn.Conv2d(64, 128, 1, 2, 0, bias=False)\n","        self.block4 = _Block(3, 128, 64, (2,1), 1)\n","        self.block5 = _Block(3, 128, 128, (1,1), 1)\n","        self.block6 = _Block(3, 128, 128, (1,1), 1)\n","        self.transition2 = nn.Conv2d(128, 256, 3, 2, 1, bias=False)\n","        self.block7 = _Block(3, 256, 128, (2,1), 1)\n","        self.block8 = _Block(3, 256, 256, (1,1), 1)\n","        self.block9 = _Block(3, 256, 256, (1,1), 1)\n","        self.transition3 = nn.Conv2d(256, 512, 3, 2, 1, bias=False)\n","        self.block10 = _Block(3, 512, 256, (2,1), 1)\n","        self.block11 = _Block(3, 512, 512, (1,1), 1)\n","        self.block12 = _Block(3, 512, 512, (1,1), 1)\n","        self.linear1 = nn.Linear(2048, n_classes)\n","    \n","    def forward(self, X):\n","        output = self.conv1(X)\n","        output = self.block1(output)\n","        output = self.block2(output)\n","        output = self.block3(output)\n","        shortcut1 = self.transition1(output)\n","        output = self.block4(output, shortcut1)\n","        output = self.block5(output)\n","        output = self.block6(output)\n","        shortcut2 = self.transition2(output)\n","        output = self.block7(output, shortcut2)\n","        output = self.block8(output)\n","        output = self.block9(output)\n","        shortcut3 = self.transition3(output)\n","        output = self.block10(output, shortcut3)\n","        output = self.block11(output)\n","        output = self.block12(output)\n","        output = output.view(-1, 2048)\n","        output = self.linear1(output)\n","        return output\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"ZOXOPrOlvn4o"},"outputs":[{"data":{"text/plain":["tensor([[ 0.8199, -1.1248,  0.1818,  1.9149, -0.8585, -0.2305, -0.8057,  1.0633,\n","          0.1171, -0.8376]], device='mps:0', grad_fn=<LinearBackward0>)"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["X = torch.ones((1,3,32,32))\n","model = ResNet(32, 10)\n","model.to(device)\n","X = X.to(device)\n","model(X)"]},{"cell_type":"markdown","metadata":{"id":"Owv2ZoaSwUzY"},"source":["# Load data and define the image transforms \n","\n","\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"f5HmmFCAauqn"},"outputs":[],"source":["train_transform = transforms.Compose([\n","    transforms.RandomHorizontalFlip(p=0.5),\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.ToTensor(),\n","    transforms.Normalize(means, stds)\n","])\n","\n","valid_transform = transforms.Compose([\n","    transforms.RandomHorizontalFlip(p=0.5),\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.ToTensor(),\n","    transforms.Normalize(means, stds)\n","])\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"HFaYPElBQHUa"},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"]}],"source":["train_set = datasets.CIFAR10(root = \"./data\", train=True, download = True, transform=train_transform)\n","\n","valid_set = datasets.CIFAR10(root = \"./data\", train=False, download = True, transform=valid_transform)\n","\n","BATCH_SIZE = 256\n","\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=5, shuffle=True)\n","\n","valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=5, shuffle=True)\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"MqNBDjhm2M1l"},"outputs":[{"name":"stdout","output_type":"stream","text":["train set is 50000 x 32 x 32 x 3\n","valid set is 10000 x 32 x 32 x 3\n"]}],"source":["print(f'train set is', ' x '.join(str(x) for x in train_set.data.shape))\n","print(f'valid set is', ' x '.join(str(x) for x in valid_set.data.shape))"]},{"cell_type":"markdown","metadata":{"id":"O0iFC4XnNJgm"},"source":["# Training the Model\n","\n"]},{"cell_type":"markdown","metadata":{"id":"l45S0m0gqBAf"},"source":["## Set up training parameters\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"cunMaY4PqBAf"},"outputs":[],"source":["loss_fn = nn.CrossEntropyLoss()\n","LR = 0.001\n","optim = torch.optim.Adam(model.parameters(), lr = LR, weight_decay=0.0001)\n","epoch_loss_DA = []\n","val_loss_DA = []\n","acc_DA = []\n","train_time = 0\n","\n","EPOCHS = 15"]},{"cell_type":"markdown","metadata":{"id":"Kx4Fl_jwqBAj"},"source":["## Train and evaluate"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"uBw2B1mMqBAk"},"outputs":[{"ename":"TypeError","evalue":"list.append() argument after * must be an iterable, not float","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m/Users/javiermunoz/Universidad/MasterDeepLearning/DL4ISP/DL4ISP/Lab1/[UAM-DL4IP] Image Data Augmentation (5).ipynb Celda 23\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/javiermunoz/Universidad/MasterDeepLearning/DL4ISP/DL4ISP/Lab1/%5BUAM-DL4IP%5D%20Image%20Data%20Augmentation%20%285%29.ipynb#X31sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     correct \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(label \u001b[39m==\u001b[39m y_b)\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/javiermunoz/Universidad/MasterDeepLearning/DL4ISP/DL4ISP/Lab1/%5BUAM-DL4IP%5D%20Image%20Data%20Augmentation%20%285%29.ipynb#X31sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m y_b\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/javiermunoz/Universidad/MasterDeepLearning/DL4ISP/DL4ISP/Lab1/%5BUAM-DL4IP%5D%20Image%20Data%20Augmentation%20%285%29.ipynb#X31sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m val_loss_DA\u001b[39m.\u001b[39;49mappend(\u001b[39m*\u001b[39;49mBATCH_SIZE\u001b[39m/\u001b[39;49m\u001b[39m10000\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/javiermunoz/Universidad/MasterDeepLearning/DL4ISP/DL4ISP/Lab1/%5BUAM-DL4IP%5D%20Image%20Data%20Augmentation%20%285%29.ipynb#X31sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m acc_DA\u001b[39m.\u001b[39mappend(\u001b[39mround\u001b[39m(correct\u001b[39m/\u001b[39mtotal,\u001b[39m2\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/javiermunoz/Universidad/MasterDeepLearning/DL4ISP/DL4ISP/Lab1/%5BUAM-DL4IP%5D%20Image%20Data%20Augmentation%20%285%29.ipynb#X31sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m[\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m:\u001b[39;00m\u001b[39m03d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m] train loss: \u001b[39m\u001b[39m{\u001b[39;00mep\u001b[39m*\u001b[39mBATCH_SIZE\u001b[39m/\u001b[39m\u001b[39m50000\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m04f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m  \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/javiermunoz/Universidad/MasterDeepLearning/DL4ISP/DL4ISP/Lab1/%5BUAM-DL4IP%5D%20Image%20Data%20Augmentation%20%285%29.ipynb#X31sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m           \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval loss: \u001b[39m\u001b[39m{\u001b[39;00mval\u001b[39m*\u001b[39mBATCH_SIZE\u001b[39m/\u001b[39m\u001b[39m10000\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m04f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m  \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/javiermunoz/Universidad/MasterDeepLearning/DL4ISP/DL4ISP/Lab1/%5BUAM-DL4IP%5D%20Image%20Data%20Augmentation%20%285%29.ipynb#X31sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m           \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval acc: \u001b[39m\u001b[39m{\u001b[39;00mcorrect\u001b[39m/\u001b[39m\u001b[39m10000\u001b[39m\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m'\u001b[39m)\n","\u001b[0;31mTypeError\u001b[0m: list.append() argument after * must be an iterable, not float"]}],"source":["for i in range(EPOCHS):\n","    start_time = time.time()\n","    ep = 0\n","    model.train()\n","    for X_b, y_b in train_loader:\n","        optim.zero_grad()\n","        X_b = X_b.to(device)\n","        y_b = y_b.to(device)\n","        output = model(X_b)\n","        loss = loss_fn(output, y_b)\n","        loss.backward()\n","        ep += loss.item()\n","        optim.step()\n","    epoch_loss_DA.append(ep*BATCH_SIZE/50000)\n","    train_time += time.time() - start_time\n","    \n","    correct = 0\n","    total = 0\n","    val = 0\n","    model.eval()\n","    \n","    for X_b, y_b in valid_loader:\n","        X_b = X_b.to(device)\n","        y_b = y_b.to(device)\n","        output = model(X_b)\n","        loss = loss_fn(output, y_b)\n","        val += loss.item()\n","        probs = torch.functional.F.softmax(output, 1)\n","        label = torch.argmax(probs, dim=1)\n","        correct += torch.sum(label == y_b).item()\n","        total += y_b.shape[0]\n","    val_loss_DA.append(val*BATCH_SIZE/10000)\n","    acc_DA.append(round(correct/total,2))\n","    \n","    print(f'[{i:03d}] train loss: {ep*BATCH_SIZE/50000:04f}  '\n","              f'val loss: {val*BATCH_SIZE/10000:04f}  '\n","              f'val acc: {correct/10000*100:.4f}%')\n","    \n","print(\"--- %s seconds ---\", train_time)\n","\n","        \n","\n"]},{"cell_type":"markdown","metadata":{"id":"TIqy6szZqBAn"},"source":["## Plot graphs\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hBu5gcKrqBAn"},"outputs":[],"source":["fig, ax = plt.subplots(figsize=(15, 8))\n","plt.plot(range(EPOCHS), epoch_loss_DA , color='r')\n","plt.plot(range(EPOCHS), val_loss_DA, color='b')\n","plt.legend([\"Train Loss\", \"Validation Loss\"])\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss\")\n","ax.grid(True)\n","\n","fig, ax = plt.subplots(figsize=(15, 8))\n","plt.plot(range(EPOCHS), acc_DA , color='g')\n","plt.legend([\"Validation Accuracy\"])\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Accuracy\")\n","ax.grid(True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"[UAM-DL4IP] Image Data Augmentation (2).ipynb","provenance":[{"file_id":"1zookUxeBLq2zqDa2W4rG_r3b4nYmYkhg","timestamp":1603444666656},{"file_id":"1QIRmlCA-LtlX5wV4oSx4A9eIZxfUb1QI","timestamp":1603049021566},{"file_id":"1dl2xGhhuRL1CXR1ts207qzF8wVHSWlJc","timestamp":1603040255366},{"file_id":"1wAXQg0DIlaFIElZWSUqu4YUjpK16qp0A","timestamp":1602254884643},{"file_id":"https://github.com/bentrevett/pytorch-image-classification/blob/master/3_alexnet.ipynb","timestamp":1599039771698}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":0}
