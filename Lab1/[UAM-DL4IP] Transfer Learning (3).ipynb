{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[UAM-DL4IP] Transfer Learning (1).ipynb","provenance":[{"file_id":"1VpCULyseIIQi1UchBAJ3FeO2u8uy2nzX","timestamp":1603443481254},{"file_id":"1Y7KtcfrEQpjH6InzS0WDbPvJq4ImDNcv","timestamp":1601559167205},{"file_id":"1ontq6LVi7Oyq5cwT4tUzYhGnjF_0Xl0t","timestamp":1598957237289},{"file_id":"1wjZfZLUHBgsWzle1tQyCvpZfKkcwvgot","timestamp":1586961875905},{"file_id":"1Rpd2rGuAnCKvIwguWMSKb0nu2a6kNmeg","timestamp":1585846674800},{"file_id":"1YByBWIYlZwRMD9cnr7ysaIw8EcnrMiLj","timestamp":1585830124838}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"9d55db645672457baa85bffef5d5cb79":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4f5a2bdd7f0e41f697e15a4442cc857d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_42aeba7d4cba44989153f9fe4157cf38","IPY_MODEL_e4b41b6d50274094a82c07772eda8047","IPY_MODEL_d606060d05e849f8b5d0446bf7cd7039"]}},"4f5a2bdd7f0e41f697e15a4442cc857d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"42aeba7d4cba44989153f9fe4157cf38":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1bcd2daf699f458cbc2c609da545139a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5100815a816f40e890b0e225d084cbb3"}},"e4b41b6d50274094a82c07772eda8047":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_3b686d07988e430eac964a139c5fbe01","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_736026e7378b49f08638992d8e105e7a"}},"d606060d05e849f8b5d0446bf7cd7039":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_578ee86ec79e45a5b883c4cdf895be34","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 7997312/? [00:00&lt;00:00, 18601010.89it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e3bc3b1d6d5d4154bbc28b77a8ac16a8"}},"1bcd2daf699f458cbc2c609da545139a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5100815a816f40e890b0e225d084cbb3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3b686d07988e430eac964a139c5fbe01":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"736026e7378b49f08638992d8e105e7a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":"20px","min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"578ee86ec79e45a5b883c4cdf895be34":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e3bc3b1d6d5d4154bbc28b77a8ac16a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"SG4KgPaT382H"},"source":["# Image Classification with PyTorch: Transfer learning \n","\n","[Pablo Carballeira] (http://dymas.ii.uam.es/webvpu/gti/user/186/), Escuela Politecnica Superior, Universidad Autónoma de Madrid.\n","\n","Parts of this code have been adapted from then work of Kevin McGuinness (http://www.eeng.dcu.ie/~mcguinne/), School of Electronic Engineering, Dublin City University, and the work of Ben Trevett (https://github.com/bentrevett), Heriot-Watt University\n","\n","You can find documentation about working in Colab here (https://colab.research.google.com/notebooks/intro.ipynb)\n","\n","---\n","\n","This lab will investigate the use of transfer learning for a simple binary classification problem. We will work with the \"Alien vs Predator\" dataset that that originally comes from a [Kaggle competition](https://www.kaggle.com/pmigdal/alien-vs-predator-images). The task is to classify images as containing either the Alien creature from the Ridley Scott movie, or the Predator creature from the 1987 action classic Predator starring Arnold Schwarzenegger.\n","\n","![AVP](https://crowdcast-prod.imgix.net/-Kab5fihhH1Q3R92MsOJ/event-cover-4763?w=800)\n","\n","\n","The dataset contains around 700 training examples and 200 validation examples, which are evenly balanced between the classes. However, 700 examples is quite a small dataset when comes to deep learning, and training a neural network from scratch (from a random initialization) would **overfit**.\n","\n","This notebook will look at simple transfer learning techniques:\n","- Using a pretrained network **\"off-the-shelf\"** as a feature extractor. \n","- **Fine tuning** a pretrained network for the new task.\n","\n","In addition to the accuracy results, we will look at the t-SNE representations of the trained from scratch, pre-trained and fine-tuned features to understand their capacity to separate both classes\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FGmS80CT8aCw"},"source":["# Instructions\n","\n","Anywhere you see a **???** in the code below, fill in in with the correct code."]},{"cell_type":"markdown","metadata":{"id":"7tqX_deL4KHY"},"source":["# Import packages\n","\n","\n","Find the PyTorch docs at https://pytorch.org/docs/stable/index.html \n","\n","Tutorials: https://pytorch.org/tutorials/"]},{"cell_type":"code","metadata":{"id":"hNBQ-hgC4DwD"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import sklearn.metrics as metrics\n","from sklearn import decomposition\n","from sklearn import manifold\n","\n","import torch\n","import torch.nn.functional as F\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","\n","from torchvision.datasets.utils import download_file_from_google_drive\n","from torchvision.datasets import ImageFolder\n","from torch.utils.data import DataLoader\n","from PIL import Image\n","\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","import copy\n","import random\n","import time"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-MGvs0jJ0JgA"},"source":["# Enable GPU acceleration\n","\n","Open to the Edit menu and select *Notebook settings* and check that *GPU* is selected under hardware accelerator.\n"]},{"cell_type":"code","metadata":{"id":"2OSN5zLGLxkF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635961164598,"user_tz":-60,"elapsed":24,"user":{"displayName":"Pablo Carballeira","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02896817432703187842"}},"outputId":"92565c67-f071-4f83-cbde-e50529fc94fb"},"source":["# make sure to enable GPU acceleration!\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"markdown","metadata":{"id":"1FcvzjWq-0xs"},"source":["We set the random seed so all of our experiments can be reproduced."]},{"cell_type":"code","metadata":{"id":"QfZxvR09IafJ"},"source":["SEED = 1234\n","\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HcQwW5s48flE"},"source":["# Dataset download\n","\n","Here we will use the \"Alien vs Predator\" dataset, which was originally taken from a [Kaggle competition](https://www.kaggle.com/pmigdal/alien-vs-predator-images). You can download it using this [Google Drive link](https://drive.google.com/file/d/1MFC6nE5XOdeP7xiz1XkCTJwNvHPHG524/view?usp=sharing). Here I use wget to download it directly from Google Drive."]},{"cell_type":"code","metadata":{"id":"ElGMJdw08e5w","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["9d55db645672457baa85bffef5d5cb79","4f5a2bdd7f0e41f697e15a4442cc857d","42aeba7d4cba44989153f9fe4157cf38","e4b41b6d50274094a82c07772eda8047","d606060d05e849f8b5d0446bf7cd7039","1bcd2daf699f458cbc2c609da545139a","5100815a816f40e890b0e225d084cbb3","3b686d07988e430eac964a139c5fbe01","736026e7378b49f08638992d8e105e7a","578ee86ec79e45a5b883c4cdf895be34","e3bc3b1d6d5d4154bbc28b77a8ac16a8"]},"executionInfo":{"status":"ok","timestamp":1635961319374,"user_tz":-60,"elapsed":1613,"user":{"displayName":"Pablo Carballeira","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02896817432703187842"}},"outputId":"6131e270-520e-4604-9548-7b0c697410df"},"source":["!mkdir data\n","!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1MFC6nE5XOdeP7xiz1XkCTJwNvHPHG524' -O data/avp.tar\n"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9d55db645672457baa85bffef5d5cb79","version_minor":0,"version_major":2},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"lBj-0y6ZqrhD"},"source":["Extract the data and remove the tar file"]},{"cell_type":"code","metadata":{"id":"scFItwaI9GoX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635961305151,"user_tz":-60,"elapsed":641,"user":{"displayName":"Pablo Carballeira","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02896817432703187842"}},"outputId":"3f3af07d-2ded-469d-bdfe-e0f223618fc5"},"source":["#!unzip data/avp.zip -d data\n","#!rm data/avp.zip\n","\n","!cd data && tar xf avp.tar\n","!rm data/avp.tar\n","\n","!ls data/avp"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tar: This does not look like a tar archive\n","tar: Skipping to next header\n","tar: Exiting with failure status due to previous errors\n","ls: cannot access 'data/avp': No such file or directory\n"]}]},{"cell_type":"markdown","metadata":{"id":"ArG6qGSy5SFT"},"source":["# Create the dataset\n","\n","We will use the `ImageFolder` class to create the dataset. This works whenever we have a folder with subfolders, each of which represents a class. You can explore the `./content/data/avp` folder to check the structure of directories\n"]},{"cell_type":"markdown","metadata":{"id":"nqfXXo6WoPYh"},"source":["First, let's take a look at some examples from the training set, to see how these images look like"]},{"cell_type":"code","metadata":{"id":"UrcecPD3nf8Y","colab":{"base_uri":"https://localhost:8080/","height":374},"executionInfo":{"status":"error","timestamp":1635961171371,"user_tz":-60,"elapsed":785,"user":{"displayName":"Pablo Carballeira","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02896817432703187842"}},"outputId":"7209e917-6865-4400-9b99-66701f6c63ff"},"source":["visual_train_set = ImageFolder('data/avp/train')\n","for i in range(5):\n","    plt.figure()\n","    plt.imshow(visual_train_set[i][0])\n","    plt.title(visual_train_set.classes[visual_train_set[i][1]])\n","    plt.axis('off')\n","    plt.show()\n","\n","    plt.figure()\n","    plt.imshow(visual_train_set[i+400][0])\n","    plt.title(visual_train_set.classes[visual_train_set[i+400][1]])\n","    plt.axis('off')\n","    plt.show()"],"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-061ee50ff548>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvisual_train_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/avp/train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisual_train_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisual_train_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvisual_train_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    311\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m                                           is_valid_file=is_valid_file)\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    143\u001b[0m         super(DatasetFolder, self).__init__(root, transform=transform,\n\u001b[1;32m    144\u001b[0m                                             target_transform=target_transform)\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \"\"\"\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \"\"\"\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/avp/train'"]}]},{"cell_type":"markdown","metadata":{"id":"fm23eka_fmOz"},"source":["We can see that this dataset is composed by images with different sizes and aspect ratios. We have to take this into account when creating the training and validation samples, as CNNs are usually designed for square images of fixed-size."]},{"cell_type":"markdown","metadata":{"id":"xX_9jPOKwe5p"},"source":["# Define the transformations"]},{"cell_type":"markdown","metadata":{"id":"UMmKGOFAfQbJ"},"source":["Here, we specify the set of transforms to be applied to the data. In addition to normalization, we apply image resizing and cropping to adapt the sizes of the images to the input of the CNN. For the time being, we are going to:\n","* resize the images by resizing the smallest dimension to 192 pixels (maintaining the aspect ratio), and\n","* take a  192x192 crop in from the image center.\n","\n","Later, we will see more efficient ways to do this image cropping in the training and validation data. "]},{"cell_type":"code","metadata":{"id":"uqJ936zY5Twi"},"source":["# Resize image and take a center crop. Check torchvision help to find the correct functions\n","train_transform = transforms.Compose([\n","    ???\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))                  \n","])\n","\n","# apply, the same transformation to the validation data\n","valid_transform = ???\n","\n","# Create training and validation sets using ImageFolder, and applying the \n","# transformation defined above\n","train_set = ??? \n","valid_set = ???\n","\n","# Create training and validation loaders\n","train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n","valid_loader = DataLoader(valid_set, batch_size=64, shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iuSFngxf6Yqu"},"source":["print(f'train samples: {len(train_set)}  validation samples: {len(valid_set)}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jreyqYn8_gzs"},"source":["# Training from scratch\n","\n","First of all, let's see what happens if we train a CNN from scratch using the Alien vs Predator dataset.\n","\n","Let's use the ResNet-50 model described by Kaiming He in his seminal paper [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385). We will load the model, with random weights, and take a look at its layers"]},{"cell_type":"markdown","metadata":{"id":"oFmQC_I6pOGr"},"source":["## Load the model and modify last FC layer"]},{"cell_type":"code","metadata":{"id":"CdUueQKukrLL"},"source":["# create a resnet50 model as before but do NOT load pre-trained weights\n","model = models.resnet50(pretrained=False)\n","model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8R244XtzkmrT"},"source":["This model architecture is defined to be trained in the ImageNet dataset (as in the vast majority of cases). Therefore, the last FC layer is set to output scores for the 1000 classes of the ImageNet dataset. The first thing we need to do is to change the last FC layer so it outputs the scores of our two classes (Alien and Predator)"]},{"cell_type":"code","metadata":{"id":"r2dlBB0nBlLb"},"source":["# Replace the fc layer of the model with a FC layer of appropriate size\n","model.fc = ???\n","\n","model.to(device)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VWAxUaw-pXw3"},"source":["## Training the model"]},{"cell_type":"markdown","metadata":{"id":"O95hgVNXmgra"},"source":["Let's define the training functions ..."]},{"cell_type":"code","metadata":{"id":"YdX-_-3LCeOs"},"source":["criterion = nn.CrossEntropyLoss()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vpx7HCnXRKoM"},"source":["def save_checkpoint(optimizer, model, epoch, filename):\n","    checkpoint_dict = {\n","        'optimizer': optimizer.state_dict(),\n","        'model': model.state_dict(),\n","        'epoch': epoch\n","    }\n","    torch.save(checkpoint_dict, filename)\n","\n","\n","def load_checkpoint(optimizer, model, filename):\n","    checkpoint_dict = torch.load(filename)\n","    epoch = checkpoint_dict['epoch']\n","    model.load_state_dict(checkpoint_dict['model'])\n","    if optimizer is not None:\n","        optimizer.load_state_dict(checkpoint_dict['optimizer'])\n","    return epoch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KROX1zxFDsxm"},"source":["!mkdir -p checkpoints"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wazziTd2Cm2F"},"source":["def train_for_epoch(optimizer):\n","\n","    # put the model in training mode\n","    model.train()\n","\n","    train_losses = []\n","\n","    for batch, target in train_loader:\n","\n","        # data to GPU\n","        batch = batch.to(device)\n","        target = target.to(device)\n","\n","        # reset optimizer\n","        optimizer.zero_grad()\n","\n","        # forward pass\n","        predictions = model(batch)\n","\n","        # calculate loss\n","        loss = criterion(predictions, target)\n","\n","        # backward pass\n","        loss.backward()\n","\n","        # parameter update\n","        optimizer.step()\n","\n","        # track loss\n","        train_losses.append(float(loss.item()))\n","\n","    train_losses = np.array(train_losses)\n","    train_loss = np.mean(train_losses)  \n","\n","    return train_loss\n","\n","\n","def validate():\n","    model.eval()\n","\n","    valid_losses = []\n","    y_true, y_pred = [], []\n","\n","    with torch.no_grad():\n","        for batch, target in valid_loader:\n","\n","            # move data to the device\n","            batch = batch.to(device)\n","            target = target.to(device)\n","\n","            # make predictions\n","            predictions = model(batch)\n","\n","            # calculate loss\n","            loss = criterion(predictions, target)\n","\n","            # track losses and predictions\n","            valid_losses.append(float(loss.item()))\n","            y_true.extend(target.cpu().numpy())\n","            y_pred.extend(predictions.argmax(dim=1).cpu().numpy())\n","    \n","    y_true = np.array(y_true)\n","    y_pred = np.array(y_pred)\n","    valid_losses = np.array(valid_losses)\n","\n","    # calculate validation accuracy from y_true and y_pred\n","    valid_accuracy = metrics.accuracy_score(y_true, y_pred)\n","\n","    # calculate the mean validation loss\n","    valid_loss = np.mean(valid_losses)\n","\n","    return valid_loss, valid_accuracy\n","\n","\n","# training loop\n","def train(epochs, optimizer, first_epoch=1):\n","\n","    train_losses, valid_losses = [],  []\n","\n","    for epoch in range(first_epoch, epochs+first_epoch):\n","\n","        # train\n","        train_loss = train_for_epoch(optimizer)\n","\n","        # validation\n","        valid_loss, valid_acc = validate()\n","\n","        train_losses.append(train_loss)\n","        valid_losses.append(valid_loss)\n","    \n","        # print status\n","        print(f'[{epoch:02d}] train loss: {train_loss.mean():0.04f}  '\n","              f'valid loss: {valid_loss:0.04f}  '\n","              f'acc: {valid_acc:0.04f}')\n","        \n","        # save checkpoint\n","        save_checkpoint(optimizer, model, epoch, f'checkpoints/avp_{epoch:03d}.pkl')\n","        #torch.save(model, f'checkpoints/avp_{epoch:03d}.pkl')\n","\n","    return train_losses, valid_losses\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"02H0F-BdmqDR"},"source":["... and train the network for 15 epochs"]},{"cell_type":"code","metadata":{"id":"9c2vf1Ly_8bA"},"source":["# Create an SGD optimizer with learning rate 0.01 and momentum 0.9\n","optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","\n","# Now train the model using the above optimizer for 15 epochs\n","train_losses, valid_losses = train(15, optimizer)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Urau1xFIKtFY"},"source":["## Plot the learning curves\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"dzwmuvqrzXpE"},"source":["epochs = range(1, len(train_losses) + 1)\n","\n","plt.figure(figsize=(10,6))\n","plt.plot(epochs, train_losses, '-o', label='Training loss')\n","plt.plot(epochs, valid_losses, '-o', label='Validation loss')\n","\n","plt.legend()\n","plt.title('Learning curves')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.xticks(epochs)\n","plt.ylim(0,5)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V8bcZ00SnGpP"},"source":["We obtain an ~75% accuracy, which is not bad, but we will see that we can do much better using transfer learning"]},{"cell_type":"markdown","metadata":{"id":"ahtWY4LzQL43"},"source":["## Features trained from scratch: t-SNE representation\n","\n","Let's take a look at the t-SNE representations of the features of the model. The features are obtained just before the FC layer (remember that the ResNet models include a Global Average Pooling layer followed by a single FC layer)\n","\n","We want to check their representation capacity, in order to correctly separate both classes, and we will compare these features trained from scratch with the pre-trained and fine-tuned features. \n","\n","First, we define the functions for the t-SNE representation"]},{"cell_type":"code","metadata":{"id":"plZRbvc1riVT"},"source":["def get_representations(model, iterator, device):\n","\n","    model.eval()\n","\n","    outputs = []\n","    intermediates = []\n","    labels = []\n","\n","    with torch.no_grad():\n","        \n","        for (x, y) in iterator:\n","\n","            x = x.to(device)\n","\n","            y_pred = model(x)\n","\n","            outputs.append(y_pred.cpu())\n","            labels.append(y)\n","        \n","    outputs = torch.cat(outputs, dim = 0)\n","    labels = torch.cat(labels, dim = 0)\n","\n","    return outputs, labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3mnBYh1kd_F_"},"source":["def plot_representations(data, labels, classes, n_images = None):\n","    \n","    if n_images is not None:\n","        data = data[:n_images]\n","        labels = labels[:n_images]\n","        \n","    fig = plt.figure(figsize = (10, 10))\n","    ax = fig.add_subplot(111)\n","    scatter = ax.scatter(data[:, 0], data[:, 1], c = labels, cmap = 'tab10')\n","    handles, labels = scatter.legend_elements()\n","    legend = ax.legend(handles = handles, labels = classes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"duqrChdkd_GD"},"source":["def get_tsne(data, n_components = 2, n_images = None):\n","    \n","    if n_images is not None:\n","        data = data[:n_images]\n","        \n","    tsne = manifold.TSNE(n_components = n_components, random_state = 0)\n","    tsne_data = tsne.fit_transform(data)\n","    return tsne_data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"51O3akKL1Tbq"},"source":["Get the features and labels of the samples of the validation set.\n","\n","The first thing we need to do is to remove the last fully connected layer. An empty `nn.Sequential` layer can be used as an Identity mapping. \n","\n"]},{"cell_type":"code","metadata":{"id":"LstHVGf27I4i"},"source":["model.fc = ???"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wh95C7fB-rUG"},"source":["Now obtain the features and labels for the validation samples using the functions defined above"]},{"cell_type":"code","metadata":{"id":"3vPRW_BIrkQT"},"source":["valid_features, valid_labels = ???"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T7Mtu1Vx1Mwu"},"source":["Plot the t-SNE representations"]},{"cell_type":"code","metadata":{"id":"fdZGF_76d_GF"},"source":["N_IMAGES = 200\n","\n","output_tsne_data = get_tsne(valid_features, n_images = N_IMAGES)\n","plot_representations(output_tsne_data, valid_labels, valid_set.classes, n_images = N_IMAGES)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t5XHNFrU6ikS"},"source":["# Load a pre-trained ResNet model\n","\n","Now let's load a pretrained ResNet50 model and try some transfer learning approaches\n"]},{"cell_type":"code","metadata":{"id":"clk9I7YZ6c8k"},"source":["model = models.resnet50(pretrained=True)\n","model.to(device);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RjakAahZ7LsJ"},"source":["# Off-the-shelf transfer learning\n","\n","Let's start with \"off-the-shelf\" transfer learning. Here we will just use the pretrained model as a feature extractor and then train a linear classifier on top of these pretrained features.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kCihnKsjBNEg"},"source":["## Pre-trained features: t-SNE representation"]},{"cell_type":"code","metadata":{"id":"QGAl5VmXzkMd"},"source":["# use the code above to extract the pre-trained features (before FC layer) and plot\n","# its t-SNE representation in two dimensions. We recommend that you encapsulate the code \n","# in a single cell. You will reuse it later\n","???"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OI-RpIalBZhb"},"source":["## Pre-trained features for training and validation data"]},{"cell_type":"code","metadata":{"id":"BQRn3x_lBahA"},"source":["# extraction of training and validation features and labels\n","train_features, train_labels = ??? \n","valid_features, valid_labels = ??? \n","\n","print(f'train features are {train_features.shape}')\n","print(f'valid features are {valid_features.shape}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G6QCH9PczkCA"},"source":["## Train a Support Vector Machine on top of the pre-trained features"]},{"cell_type":"markdown","metadata":{"id":"3YRGhvTi9fYi"},"source":["Now, train a classical machine learning model on the representations. Let's try a **support vector machine** with a linear kernel as the classifier."]},{"cell_type":"code","metadata":{"id":"ERmD9HNc-MwB"},"source":["from sklearn.svm import SVC"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XwQ1iHmT_gv7"},"source":["# define a linear SMV classifier using SVC, and use the fit function to train it using the training features\n","classifier = ???\n","???"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1ysrUER9_pk8"},"source":["# make predictions for the validation feature using the classifier trained above\n","y_hat = ???"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cn85fPGZ_sD_"},"source":["accuracy = metrics.accuracy_score(valid_labels, y_hat)\n","print(f'Accuracy: {accuracy:0.3f}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zlszcjlOmVUg"},"source":["You should get 90.5% accuracy off the shelf: not bad! The validation set is balanced, so this is an excellent start."]},{"cell_type":"code","metadata":{"id":"UFkZUT8Fhagc"},"source":["print(metrics.classification_report(valid_labels, y_hat, target_names=train_set.classes))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mrlZjrnBBLJe"},"source":["# Fine tuning\n","\n","Let's try replace the last layer of the ResNet and fine tune it on the AVP dataset for a few epochs"]},{"cell_type":"markdown","metadata":{"id":"5bbTcLzFC6wF"},"source":["## Modify the model"]},{"cell_type":"code","metadata":{"id":"xlgVtaVeBTfG"},"source":["model = models.resnet50(pretrained=True)\n","# Replace fc with an fc layer of appropuate size (same as in training from scratch)\n","model.fc = ???\n","\n","model.to(device);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0FcqPXN0C-lu"},"source":["## Train (fine-tune) the model"]},{"cell_type":"markdown","metadata":{"id":"w3ekZIA0Ex9k"},"source":["Train the final layer for a few epochs to get it to a good place (remember, it starts off with random weights but the rest of the network has pre-trained weights from ImageNet)"]},{"cell_type":"code","metadata":{"id":"CoRUK3U_whvs"},"source":["final_layer_optimizer = optim.SGD(model.fc.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n","train_losses, valid_losses = train(2, final_layer_optimizer)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0gXezKklE80_"},"source":["Now fine tune whole network with a lower learning rate"]},{"cell_type":"code","metadata":{"id":"5kXYcIg-wnL8"},"source":["full_network_optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","# train the model using the full network optimizer above for 10 epochs starting at epoch 3\n","train_losses_full, valid_losses_full =  ???\n","\n","train_losses.extend(train_losses_full)\n","valid_losses.extend(valid_losses_full)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4BMQly9UlATU"},"source":["epochs = range(1, len(train_losses) + 1)\n","\n","plt.figure(figsize=(10,6))\n","plt.plot(epochs, train_losses, '-o', label='Training loss')\n","plt.plot(epochs, valid_losses, '-o', label='Validation loss')\n","\n","plt.legend()\n","plt.title('Learning curves')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.xticks(epochs)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AE0QxzQqgKSd"},"source":["Use the load_checkpoint function to load the model with the best performance"]},{"cell_type":"code","metadata":{"id":"B09h-Rt9gI11"},"source":["???"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bLTKxBVrmHjI"},"source":["## Fine-tuned features: t-SNE representations "]},{"cell_type":"code","metadata":{"id":"t4bweXfh0JUi"},"source":["# use the code above to extract the fine-tuned features (before FC layer) and plot\n","# its t-SNE representation in two dimensions\n","???"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SLRgREj10eqd"},"source":["# Data augmentation\n","\n","Let's see if we can improve the performance of the fine-tuned model by applying some data augmentation to the training data. "]},{"cell_type":"markdown","metadata":{"id":"zr8M4uYh1rGl"},"source":["## Data augmentation transforms\n","\n","Now we will define a set of transforms that will be applied to the data that implement basic data augmentation techniques: Random Cropping and Random Horizontal Flipping. \n","\n","The train transform is stochastic, which will help prevent overfitting when we fine-tune the model. The validation transform is deterministic, which ensures we will have consistent results on the same data."]},{"cell_type":"code","metadata":{"id":"WXJaV77Q1rGm"},"source":["# define a transformation for the training data that resizes the image to 224x224 pixels,\n","# extracts a random 192x192 crop, and applies horizontal flip with 0.5 probability. \n","# Use the same data normalization we have used above\n","train_transform = ???\n","\n","\n","# define a similar transformation for the validation data that resizes the image to 224x224 pixels,\n","# but now extracts a 192x192 crop from the image center\n","valid_transform = ???\n","\n","# Create training and validation sets using ImageFolder, and applying the \n","# transformation defined above\n","train_set = ??? \n","valid_set = ???\n","\n","train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n","# Create a loader for the validation set, but disable shuffling of the validation data\n","# (it is unnecessary to shuffle validation data)\n","valid_loader = DataLoader(valid_set, batch_size=64, shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ae4oaNMp1rGp"},"source":["print(f'train samples: {len(train_set)}  validation samples: {len(valid_set)}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9IxYo9ia2PQc"},"source":["## Fine tuning\n","\n","Use the previous code to fine-tune the pre-trained ResNet model, using the augmented training data, plot the learning graphs, and get the t-SNE representations of the features of the last layer"]},{"cell_type":"code","metadata":{"id":"zTavG5yps_dr"},"source":["???"],"execution_count":null,"outputs":[]}]}